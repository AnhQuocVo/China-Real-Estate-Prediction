{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 111876,
          "databundleVersionId": 13320609,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Real Estate Demand Prediction",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnhQuocVo/China-Real-Estate-Prediction/blob/main/Real_Estate_Demand_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "-9ek5PKNrXxs"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "china_real_estate_demand_prediction_path = kagglehub.competition_download('china-real-estate-demand-prediction')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "eykyDGGvrXxt"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. IMPORTS & CONFIGURATION"
      ],
      "metadata": {
        "id": "nTiXmV7vrXxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-08T08:44:00.925271Z",
          "iopub.execute_input": "2025-10-08T08:44:00.925639Z",
          "iopub.status.idle": "2025-10-08T08:44:01.303929Z",
          "shell.execute_reply.started": "2025-10-08T08:44:00.925607Z",
          "shell.execute_reply": "2025-10-08T08:44:01.302509Z"
        },
        "id": "f-WM2ZHYrXxu",
        "outputId": "794fac5c-c133-45ee-e570-dd6d87c0998f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/china-real-estate-demand-prediction/sample_submission.csv\n/kaggle/input/china-real-estate-demand-prediction/test.csv\n/kaggle/input/china-real-estate-demand-prediction/train/city_search_index.csv\n/kaggle/input/china-real-estate-demand-prediction/train/land_transactions_nearby_sectors.csv\n/kaggle/input/china-real-estate-demand-prediction/train/new_house_transactions_nearby_sectors.csv\n/kaggle/input/china-real-estate-demand-prediction/train/city_indexes.csv\n/kaggle/input/china-real-estate-demand-prediction/train/pre_owned_house_transactions.csv\n/kaggle/input/china-real-estate-demand-prediction/train/new_house_transactions.csv\n/kaggle/input/china-real-estate-demand-prediction/train/land_transactions.csv\n/kaggle/input/china-real-estate-demand-prediction/train/sector_POI.csv\n/kaggle/input/china-real-estate-demand-prediction/train/pre_owned_house_transactions_nearby_sectors.csv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import gc\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Hyperparameter Optimization\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"Optuna not available. Using default hyperparameters.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-08T08:44:01.306352Z",
          "iopub.execute_input": "2025-10-08T08:44:01.307082Z",
          "iopub.status.idle": "2025-10-08T08:44:10.014023Z",
          "shell.execute_reply.started": "2025-10-08T08:44:01.307022Z",
          "shell.execute_reply": "2025-10-08T08:44:10.01307Z"
        },
        "id": "nQ9Ykqw-rXxv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-08T08:44:10.014862Z",
          "iopub.execute_input": "2025-10-08T08:44:10.01557Z",
          "iopub.status.idle": "2025-10-08T08:44:10.022827Z",
          "shell.execute_reply.started": "2025-10-08T08:44:10.015545Z",
          "shell.execute_reply": "2025-10-08T08:44:10.021238Z"
        },
        "id": "0OzwZfBQrXxv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. DATA LOADING & OVERVIEW"
      ],
      "metadata": {
        "id": "JPsZxP8DrXxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    # Training data\n",
        "    new_house = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/train/new_house_transactions.csv')\n",
        "    new_house_nearby = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/train/new_house_transactions_nearby_sectors.csv')\n",
        "    preowned_house = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/train/pre_owned_house_transactions.csv')\n",
        "    preowned_nearby = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/train/pre_owned_house_transactions_nearby_sectors.csv')\n",
        "    land_trans = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/train/land_transactions.csv')\n",
        "    land_nearby = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/train/land_transactions_nearby_sectors.csv')\n",
        "    sector_poi = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/train/sector_POI.csv')\n",
        "    city_search = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/train/city_search_index.csv')\n",
        "    city_indexes = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/train/city_indexes.csv')\n",
        "\n",
        "    # Test data\n",
        "    test = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/test.csv')\n",
        "    sample_submission = pd.read_csv('/kaggle/input/china-real-estate-demand-prediction/sample_submission.csv')\n",
        "\n",
        "    print(\"âœ“ All datasets loaded successfully!\")\n",
        "\n",
        "    datasets = {\n",
        "        'new_house': new_house,\n",
        "        'new_house_nearby': new_house_nearby,\n",
        "        'preowned_house': preowned_house,\n",
        "        'preowned_nearby': preowned_nearby,\n",
        "        'land_trans': land_trans,\n",
        "        'land_nearby': land_nearby,\n",
        "        'sector_poi': sector_poi,\n",
        "        'city_search': city_search,\n",
        "        'city_indexes': city_indexes,\n",
        "        'test': test,\n",
        "        'sample_submission': sample_submission\n",
        "    }\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def data_overview(datasets):\n",
        "    for name, df in datasets.items():\n",
        "        if name != 'sample_submission':\n",
        "            print(f\"\\n{name.upper()}:\")\n",
        "            print(f\"  Shape: {df.shape}\")\n",
        "            print(f\"  Columns: {list(df.columns)}\")\n",
        "            print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
        "            print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Load data\n",
        "data = load_data()\n",
        "data_overview(data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "uUTXTNkmrXxv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. EXPLORATORY DATA ANALYSIS (EDA)"
      ],
      "metadata": {
        "id": "0EZh2Jo0rXxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_month_sector(df):\n",
        "    df = df.copy()\n",
        "    df['month'] = df['month_sector'].str.extract(r'(\\d{4}\\s+\\w+)')[0]\n",
        "    df['sector'] = df['month_sector'].str.extract(r'sector\\s+(\\d+)')[0].astype(int)\n",
        "    df['month'] = pd.to_datetime(df['month'], format='%Y %b')\n",
        "    return df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-08T08:44:51.026906Z",
          "iopub.execute_input": "2025-10-08T08:44:51.027222Z",
          "iopub.status.idle": "2025-10-08T08:44:51.034465Z",
          "shell.execute_reply.started": "2025-10-08T08:44:51.0272Z",
          "shell.execute_reply": "2025-10-08T08:44:51.033285Z"
        },
        "id": "h-_K3S8prXxw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def eda_analysis(data):\n",
        "    # Parse target data\n",
        "    new_house = parse_month_sector(['new_house'])\n",
        "\n",
        "    print(\"\\n1. TARGET VARIABLE ANALYSIS\")\n",
        "    print(\"-\" * 80)\n",
        "    target = 'amount_new_house_transaction'\n",
        "    print(f\"\\nTarget: {target}\")\n",
        "    print(new_house[target].describe())\n",
        "\n",
        "    # Time series plot\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "    # Overall trend\n",
        "    monthly_avg = new_house.groupby('month')[target].mean()\n",
        "    axes[0, 0].plot(monthly_avg.index, monthly_avg.values, marker='o', linewidth=2)\n",
        "    axes[0, 0].set_title('Average New House Transactions Over Time', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Month')\n",
        "    axes[0, 0].set_ylabel('Average Transaction Amount')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Distribution\n",
        "    axes[0, 1].hist(new_house[target], bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[0, 1].set_title('Distribution of Transaction Amounts', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Transaction Amount')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Top sectors\n",
        "    sector_avg = new_house.groupby('sector')[target].mean().sort_values(ascending=False).head(10)\n",
        "    axes[1, 0].barh(sector_avg.index.astype(str), sector_avg.values, color='teal')\n",
        "    axes[1, 0].set_title('Top 10 Sectors by Average Transaction', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Average Transaction Amount')\n",
        "    axes[1, 0].set_ylabel('Sector')\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "    # Box plot by year\n",
        "    new_house['year'] = new_house['month'].dt.year\n",
        "    axes[1, 1].boxplot([new_house[new_house['year'] == y][target].dropna()\n",
        "                        for y in sorted(new_house['year'].unique())],\n",
        "                       labels=sorted(new_house['year'].unique()))\n",
        "    axes[1, 1].set_title('Transaction Amount Distribution by Year', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Year')\n",
        "    axes[1, 1].set_ylabel('Transaction Amount')\n",
        "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('eda_overview.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\nâœ“ EDA plots saved as 'eda_overview.png'\")\n",
        "\n",
        "    print(\"\\n2. KEY INSIGHTS\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"  â€¢ Total unique sectors: {new_house['sector'].nunique()}\")\n",
        "    print(f\"  â€¢ Date range: {new_house['month'].min()} to {new_house['month'].max()}\")\n",
        "    print(f\"  â€¢ Average transaction amount: {new_house[target].mean():.2f}\")\n",
        "    print(f\"  â€¢ Transaction amount std: {new_house[target].std():.2f}\")\n",
        "    print(f\"  â€¢ Coefficient of Variation: {(new_house[target].std() / new_house[target].mean()):.2%}\")\n",
        "\n",
        "# Run EDA\n",
        "eda_analysis(data)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-08T08:45:33.128063Z",
          "iopub.execute_input": "2025-10-08T08:45:33.128542Z",
          "iopub.status.idle": "2025-10-08T08:45:33.154943Z",
          "shell.execute_reply.started": "2025-10-08T08:45:33.128513Z",
          "shell.execute_reply": "2025-10-08T08:45:33.153331Z"
        },
        "id": "NmZrTRMtrXxw",
        "outputId": "00ea52de-e1c3-4380-be41-270d024ac74c"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_37/1811337935.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Run EDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0meda_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "IjYOsjEzrXxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def engineer_features(data):\n",
        "    \"\"\"Comprehensive feature engineering pipeline\"\"\"\n",
        "\n",
        "    print(\"\\nEngineering features...\")\n",
        "\n",
        "    # Parse all relevant datasets\n",
        "    new_house = parse_month_sector(data['new_house'])\n",
        "    preowned = parse_month_sector(data['preowned_house'])\n",
        "    land = parse_month_sector(data['land_trans'])\n",
        "    test_df = parse_month_sector(data['test'])\n",
        "\n",
        "    # Combine train and test for consistent feature engineering\n",
        "    new_house['is_train'] = 1\n",
        "    test_df['is_train'] = 0\n",
        "    test_df['amount_new_house_transaction'] = np.nan\n",
        "\n",
        "    combined = pd.concat([new_house, test_df], axis=0, ignore_index=True)\n",
        "    combined = combined.sort_values(['sector', 'month']).reset_index(drop=True)\n",
        "\n",
        "    print(f\"  Combined dataset shape: {combined.shape}\")\n",
        "\n",
        "    # 1. Date Features\n",
        "    print(\"  â€¢ Creating date features...\")\n",
        "    combined['year'] = combined['month'].dt.year\n",
        "    combined['month_num'] = combined['month'].dt.month\n",
        "    combined['quarter'] = combined['month'].dt.quarter\n",
        "    combined['month_sin'] = np.sin(2 * np.pi * combined['month_num'] / 12)\n",
        "    combined['month_cos'] = np.cos(2 * np.pi * combined['month_num'] / 12)\n",
        "    combined['days_in_month'] = combined['month'].dt.days_in_month\n",
        "\n",
        "    # Month index (sequential counter)\n",
        "    combined['month_index'] = (combined['year'] - combined['year'].min()) * 12 + combined['month_num']\n",
        "\n",
        "    # 2. Lag Features\n",
        "    print(\"  â€¢ Creating lag features...\")\n",
        "    lag_periods = [1, 2, 3, 6, 12]\n",
        "\n",
        "    for lag in lag_periods:\n",
        "        combined[f'lag_{lag}'] = combined.groupby('sector')['amount_new_house_transaction'].shift(lag)\n",
        "\n",
        "    # 3. Rolling Statistics\n",
        "    print(\"  â€¢ Creating rolling statistics...\")\n",
        "    for window in [3, 6, 12]:\n",
        "        combined[f'rolling_mean_{window}'] = combined.groupby('sector')['amount_new_house_transaction'].transform(\n",
        "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
        "        )\n",
        "        combined[f'rolling_std_{window}'] = combined.groupby('sector')['amount_new_house_transaction'].transform(\n",
        "            lambda x: x.shift(1).rolling(window=window, min_periods=1).std()\n",
        "        )\n",
        "\n",
        "    # 4. Expanding Statistics (cumulative)\n",
        "    print(\"  â€¢ Creating expanding statistics...\")\n",
        "    combined['expanding_mean'] = combined.groupby('sector')['amount_new_house_transaction'].transform(\n",
        "        lambda x: x.shift(1).expanding(min_periods=1).mean()\n",
        "    )\n",
        "    combined['expanding_std'] = combined.groupby('sector')['amount_new_house_transaction'].transform(\n",
        "        lambda x: x.shift(1).expanding(min_periods=1).std()\n",
        "    )\n",
        "\n",
        "    # 5. Trend Features\n",
        "    print(\"  â€¢ Creating trend features...\")\n",
        "    combined['diff_1'] = combined.groupby('sector')['amount_new_house_transaction'].diff(1)\n",
        "    combined['diff_12'] = combined.groupby('sector')['amount_new_house_transaction'].diff(12)\n",
        "    combined['pct_change_1'] = combined.groupby('sector')['amount_new_house_transaction'].pct_change(1)\n",
        "\n",
        "    # 6. Merge Pre-owned House Data\n",
        "    print(\"  â€¢ Merging pre-owned house data...\")\n",
        "    preowned_agg = preowned.groupby(['month', 'sector']).agg({\n",
        "        'amount_pre_owned_house_transaction': ['mean', 'sum', 'count'],\n",
        "        'avg_price_per_sqm_pre_owned_house': ['mean', 'std']\n",
        "    }).reset_index()\n",
        "    preowned_agg.columns = ['month', 'sector', 'preowned_amount_mean', 'preowned_amount_sum',\n",
        "                            'preowned_count', 'preowned_price_mean', 'preowned_price_std']\n",
        "\n",
        "    combined = combined.merge(preowned_agg, on=['month', 'sector'], how='left')\n",
        "\n",
        "    # 7. Merge Land Transaction Data\n",
        "    print(\"  â€¢ Merging land transaction data...\")\n",
        "    land_agg = land.groupby(['month', 'sector']).agg({\n",
        "        'amount_land_transaction': ['mean', 'sum', 'count'],\n",
        "        'avg_price_per_sqm_land': ['mean', 'std']\n",
        "    }).reset_index()\n",
        "    land_agg.columns = ['month', 'sector', 'land_amount_mean', 'land_amount_sum',\n",
        "                        'land_count', 'land_price_mean', 'land_price_std']\n",
        "\n",
        "    combined = combined.merge(land_agg, on=['month', 'sector'], how='left')\n",
        "\n",
        "    # 8. Create Ratio Features\n",
        "    print(\"  â€¢ Creating ratio features...\")\n",
        "    combined['new_to_preowned_ratio'] = combined['lag_1'] / (combined['preowned_amount_mean'] + 1)\n",
        "    combined['land_to_new_ratio'] = combined['land_amount_mean'] / (combined['lag_1'] + 1)\n",
        "\n",
        "    # 9. Merge Sector POI Data\n",
        "    print(\"  â€¢ Merging sector POI data...\")\n",
        "    sector_poi = data['sector_poi']\n",
        "    if 'sector' in sector_poi.columns:\n",
        "        poi_features = sector_poi.copy()\n",
        "        combined = combined.merge(poi_features, on='sector', how='left')\n",
        "\n",
        "    # 10. Merge City-level Data\n",
        "    print(\"  â€¢ Merging city-level data...\")\n",
        "    # Note: Assumes city can be mapped from sector or is in the data\n",
        "    # This is a placeholder - adjust based on actual data structure\n",
        "\n",
        "    # 11. Target Encoding for Sector\n",
        "    print(\"  â€¢ Creating target encoding...\")\n",
        "    sector_mean = combined[combined['is_train'] == 1].groupby('sector')['amount_new_house_transaction'].mean()\n",
        "    combined['sector_target_mean'] = combined['sector'].map(sector_mean)\n",
        "\n",
        "    # 12. Sector Statistics\n",
        "    sector_stats = combined[combined['is_train'] == 1].groupby('sector')['amount_new_house_transaction'].agg([\n",
        "        'mean', 'std', 'min', 'max', 'count'\n",
        "    ]).reset_index()\n",
        "    sector_stats.columns = ['sector', 'sector_mean', 'sector_std', 'sector_min', 'sector_max', 'sector_count']\n",
        "    combined = combined.merge(sector_stats, on='sector', how='left')\n",
        "\n",
        "    print(f\"\\nâœ“ Feature engineering complete! Final shape: {combined.shape}\")\n",
        "    print(f\"  Total features: {combined.shape[1]}\")\n",
        "\n",
        "    return combined\n",
        "\n",
        "# Engineer features\n",
        "df_features = engineer_features(data)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "7odeQ8ISrXxx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Preprocess and split data\"\"\"\n",
        "\n",
        "    print(\"\\nPreprocessing data...\")\n",
        "\n",
        "    # Split back to train and test\n",
        "    train = df[df['is_train'] == 1].copy()\n",
        "    test = df[df['is_train'] == 0].copy()\n",
        "\n",
        "    print(f\"  Train shape: {train.shape}\")\n",
        "    print(f\"  Test shape: {test.shape}\")\n",
        "\n",
        "    # Define feature columns (exclude target, identifiers, and datetime)\n",
        "    exclude_cols = ['month_sector', 'month', 'amount_new_house_transaction', 'is_train']\n",
        "    feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
        "\n",
        "    print(f\"\\n  Total feature columns: {len(feature_cols)}\")\n",
        "\n",
        "    # Handle missing values\n",
        "    print(\"\\n  Handling missing values...\")\n",
        "\n",
        "    # For numeric columns, fill with median\n",
        "    numeric_cols = train[feature_cols].select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        median_val = train[col].median()\n",
        "        train[col].fillna(median_val, inplace=True)\n",
        "        test[col].fillna(median_val, inplace=True)\n",
        "\n",
        "    # For categorical columns, fill with mode\n",
        "    categorical_cols = train[feature_cols].select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        mode_val = train[col].mode()[0] if len(train[col].mode()) > 0 else 'Unknown'\n",
        "        train[col].fillna(mode_val, inplace=True)\n",
        "        test[col].fillna(mode_val, inplace=True)\n",
        "\n",
        "        # Label encode\n",
        "        le = LabelEncoder()\n",
        "        train[col] = le.fit_transform(train[col].astype(str))\n",
        "        test[col] = le.transform(test[col].astype(str))\n",
        "\n",
        "    print(f\"  Missing values in train: {train[feature_cols].isnull().sum().sum()}\")\n",
        "    print(f\"  Missing values in test: {test[feature_cols].isnull().sum().sum()}\")\n",
        "\n",
        "    # Create time-based validation split (last 6 months for validation)\n",
        "    train = train.sort_values('month')\n",
        "    split_date = train['month'].max() - pd.DateOffset(months=6)\n",
        "\n",
        "    train_data = train[train['month'] < split_date]\n",
        "    val_data = train[train['month'] >= split_date]\n",
        "\n",
        "    X_train = train_data[feature_cols]\n",
        "    y_train = train_data['amount_new_house_transaction']\n",
        "\n",
        "    X_val = val_data[feature_cols]\n",
        "    y_val = val_data['amount_new_house_transaction']\n",
        "\n",
        "    X_test = test[feature_cols]\n",
        "\n",
        "    print(f\"\\n  Time-based split:\")\n",
        "    print(f\"    Training: {X_train.shape[0]} samples (before {split_date.strftime('%Y-%m')})\")\n",
        "    print(f\"    Validation: {X_val.shape[0]} samples (from {split_date.strftime('%Y-%m')})\")\n",
        "    print(f\"    Test: {X_test.shape[0]} samples\")\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, test, feature_cols\n",
        "\n",
        "# Preprocess data\n",
        "X_train, y_train, X_val, y_val, X_test, test_df, features = preprocess_data(df_features)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "kuaN_0V5rXxx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. MODEL DEVELOPMENT"
      ],
      "metadata": {
        "id": "v4PzuQkurXxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_mape(y_true, y_pred):\n",
        "    \"\"\"Calculate MAPE (Mean Absolute Percentage Error)\"\"\"\n",
        "    mask = y_true != 0\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "def train_lightgbm(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Train LightGBM model\"\"\"\n",
        "    print(\"\\n1. Training LightGBM...\")\n",
        "\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'mape',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'verbose': -1,\n",
        "        'random_state': RANDOM_SEED\n",
        "    }\n",
        "\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=1000,\n",
        "        valid_sets=[train_data, val_data],\n",
        "        valid_names=['train', 'valid'],\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(100)]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_xgboost(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Train XGBoost model\"\"\"\n",
        "    print(\"\\n2. Training XGBoost...\")\n",
        "\n",
        "    params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'eval_metric': 'mape',\n",
        "        'max_depth': 6,\n",
        "        'learning_rate': 0.05,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'random_state': RANDOM_SEED\n",
        "    }\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "    model = xgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=1000,\n",
        "        evals=[(dtrain, 'train'), (dval, 'valid')],\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=100\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_catboost(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Train CatBoost model\"\"\"\n",
        "    print(\"\\n3. Training CatBoost...\")\n",
        "\n",
        "    model = CatBoostRegressor(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.05,\n",
        "        depth=6,\n",
        "        loss_function='MAPE',\n",
        "        random_seed=RANDOM_SEED,\n",
        "        verbose=100,\n",
        "        early_stopping_rounds=50\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        use_best_model=True\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train models\n",
        "models = {}\n",
        "models['lgb'] = train_lightgbm(X_train, y_train, X_val, y_val)\n",
        "models['xgb'] = train_xgboost(X_train, y_train, X_val, y_val)\n",
        "models['catboost'] = train_catboost(X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "trusted": true,
        "id": "KtPMYxosrXxy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. MODEL EVALUATION"
      ],
      "metadata": {
        "id": "rD5sPe-UrXxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_models(models, X_val, y_val):\n",
        "    \"\"\"Evaluate all models\"\"\"\n",
        "\n",
        "    results = {}\n",
        "    predictions = {}\n",
        "\n",
        "    print(\"\\nModel Performance on Validation Set:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        if name == 'lgb':\n",
        "            y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "        elif name == 'xgb':\n",
        "            dval = xgb.DMatrix(X_val)\n",
        "            y_pred = model.predict(dval)\n",
        "        else:  # catboost\n",
        "            y_pred = model.predict(X_val)\n",
        "\n",
        "        # Ensure non-negative predictions\n",
        "        y_pred = np.maximum(y_pred, 0)\n",
        "\n",
        "        mape = custom_mape(y_val.values, y_pred)\n",
        "        r2 = r2_score(y_val.values, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val.values, y_pred))\n",
        "\n",
        "        results[name] = {'MAPE': mape, 'R2': r2, 'RMSE': rmse}\n",
        "        predictions[name] = y_pred\n",
        "\n",
        "        print(f\"\\n{name.upper()}:\")\n",
        "        print(f\"  MAPE: {mape:.4f}%\")\n",
        "        print(f\"  RÂ²: {r2:.4f}\")\n",
        "        print(f\"  RMSE: {rmse:.2f}\")\n",
        "\n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    for idx, (name, y_pred) in enumerate(predictions.items()):\n",
        "        axes[idx].scatter(y_val, y_pred, alpha=0.5, s=10)\n",
        "        axes[idx].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()],\n",
        "                      'r--', lw=2, label='Perfect Prediction')\n",
        "        axes[idx].set_xlabel('Actual', fontsize=12)\n",
        "        axes[idx].set_ylabel('Predicted', fontsize=12)\n",
        "        axes[idx].set_title(f'{name.upper()} - RÂ²: {results[name][\"R2\"]:.4f}',\n",
        "                           fontsize=14, fontweight='bold')\n",
        "        axes[idx].legend()\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_predictions.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\nâœ“ Prediction plots saved as 'model_predictions.png'\")\n",
        "\n",
        "    return results, predictions\n",
        "\n",
        "# Evaluate models\n",
        "eval_results, val_predictions = evaluate_models(models, X_val, y_val)\n",
        "\n",
        "# Feature Importance (LightGBM)\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"TOP 20 MOST IMPORTANT FEATURES (LightGBM)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': features,\n",
        "    'importance': models['lgb'].feature_importance(importance_type='gain')\n",
        "}).sort_values('importance', ascending=False).head(20)\n",
        "\n",
        "print(importance_df.to_string(index=False))\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(importance_df['feature'], importance_df['importance'])\n",
        "plt.xlabel('Importance (Gain)', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.title('Top 20 Feature Importance (LightGBM)', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "print(\"\\nâœ“ Feature importance plot saved as 'feature_importance.png'\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "UQ3wcK7JrXxy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle competitions submit -c china-real-estate-demand-prediction -f submission.csv -m \"submit\""
      ],
      "metadata": {
        "id": "EJMRtKwFrr_3",
        "outputId": "1c51917e-bde5-4959-a26b-a2ceaaca2bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-46244318.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-46244318.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    kaggle competitions submit -c china-real-estate-demand-prediction -f submission.csv -m \"Message\"\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}